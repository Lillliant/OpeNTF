{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is OpeNTF?\n",
    "OpeNTF is an open-source framework hosting large-scale training datasets and canonical neural team formation models that are trained using fairness-aware and time-sensitive methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite for OpeNTF\n",
    "Before using OpeNTF, the following libraries are needed as a prerequisite, in addition to the libraries in `requirements.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install torch==1.9.0\n",
    "pip install pytrec-eval-terrier==0.5.2\n",
    "pip install gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "git clone --recursive https://github.com/Fani-Lab/opentf\n",
    "cd opentf\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart on OpeNTF\n",
    "\n",
    "OpeNTF has the following required arguments:\n",
    "\n",
    "- `-data`: the path of the input datasets.\n",
    "- `-domain`: the domain the input dataset belongs in.\n",
    "- `-model`: the neural team formation models to be used in the run.\n",
    "\n",
    "As well, other optional arguments include:\n",
    "- `-attribute`: the set of our sensitive attributes (e.g., popularity).\n",
    "- `-fairness`: fairness metrics for reranking algorithms, used to minimize popularity bias.\n",
    "- `-np-ratio`: desired ratio of non-popular experts after reranking.\n",
    "- `-k_max`: cutoff for the reranking algorithms.\n",
    "- `-filter`: remove outliers, if needed.\n",
    "- `-future`: predict future, if needed.\n",
    "- `-exp_id`: ID of the experiment.\n",
    "- `-output`: path of the baseline output.\n",
    "\n",
    "The following is a sample run of the OpeNTF codebase using a toy dataset `toy.dblp.v12.json`, which is modelled after the DBLP dataset: a dataset consisting of authorship and skill information on more than 4 million Computer Science research publications. Two neural models (`feedforward` and `Bayesian`) are used in this quickstart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cd src\n",
    "python -u main.py -data ../data/raw/dblp/toy.dblp.v12.json -domain dblp -model fnn bnn -fairness det_greedy -attribute popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Setting Parameters\n",
    "OpeNTF's codebase offers the following parameter to be set for each neural team formation methods:\n",
    "\n",
    "### `model`\n",
    "- Contains the baseline hyperparameters in the form of `'model-name' : { params }`, which allows the models to be integrated into the baseline with their unique parameters.\n",
    "- Allows the customization of which stages of the system to be executed through `cmd`.\n",
    "- Contains other training parameter for the models.\n",
    "\n",
    "### `data`\n",
    "- Contains parameters for manipulating datasets, including dataset filters (e.g., minimum team size) and bucket size for sparse matrix parallel generation.\n",
    "\n",
    "### `fair`\n",
    "- Contains parameters for the fairness metrics used in consideration during team formation.\n",
    "\n",
    "A snippet of the parameters used in `param.py` is displayed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "settings = {\n",
    "    'model':{\n",
    "        'baseline': {\n",
    "            'random': {\n",
    "                'b': 128\n",
    "            },\n",
    "            'fnn':{\n",
    "                'l': [100],  # list of number of nodes in each layer\n",
    "                'lr': 0.001,  # learning rate\n",
    "                'b': 128,  # batch size\n",
    "                'e': 10,  # epoch\n",
    "                'nns': 3,  # number of negative samples\n",
    "                'ns': 'none',  # 'none', 'uniform', 'unigram', 'unigram_b'\n",
    "                'loss': 'SL',  # 'SL'-> superloss, 'DP' -> Data Parameters, 'normal' -> Binary Cross Entropy\n",
    "            },\n",
    "            'bnn':{\n",
    "                'l': [128],  # list of number of nodes in each layer\n",
    "                'lr': 0.1,  # learning rate\n",
    "                'b': 128,  # batch size\n",
    "                'e': 5,  # epoch\n",
    "                'nns': 3,  # number of negative samples\n",
    "                'ns': 'unigram_b',  # 'uniform', 'unigram', 'unigram_b'\n",
    "                's': 1,  # # sample_elbo for bnn\n",
    "                'loss': 'SL',  # 'SL'-> superloss, 'DP' -> Data Parameters, 'normal' -> Binary Cross Entropy\n",
    "            },\n",
    "        },\n",
    "        'cmd': ['train', 'test', 'eval', 'fair'],  # 'train', 'test', 'eval', 'plot', 'agg', 'fair'\n",
    "        'nfolds': 3,\n",
    "        'train_test_split': 0.85,\n",
    "        'step_ahead': 2,#for now, it means that whatever are in the last [step_ahead] time interval will be the test set!\n",
    "    },\n",
    "    'data':{\n",
    "        'domain': {\n",
    "            'dblp':{},\n",
    "            'uspt':{},\n",
    "            'imdb':{},\n",
    "        },\n",
    "        'location_type': 'country', #should be one of 'city', 'state', 'country' and represents the location of members in teams (not the location of teams)\n",
    "        'filter': {\n",
    "            'min_nteam': 5,\n",
    "            'min_team_size': 2,\n",
    "        },\n",
    "        'parallel': 1,\n",
    "        'ncore': 0,# <= 0 for all\n",
    "        'bucket_size': 1000\n",
    "    },\n",
    "    'fair': {'np_ratio': None,\n",
    "              'fairness': ['det_greedy',],\n",
    "              'k_max': None,\n",
    "              'fairness_metrics': {'ndkl'},\n",
    "              'utility_metrics': {'map_cut_2,5,10'},\n",
    "              'eq_op': False,\n",
    "              'mode': 0,\n",
    "              'core': -1,\n",
    "              'attribute': ['gender', 'popularity']},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure and Inheritance\n",
    "\n",
    "### Dataset Structure\n",
    "<p align=\"center\"><img src='../../../../src/cmn/dataset_hierarchy.png' width=\"500\" ></p>\n",
    "\n",
    "To integrate a new dataset into the baseline, follow the structure of the `team` class. Additional fields can be added, like its derived classes. Ideally, only the `read_data()` function should be overriden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "[\n",
    "  {\n",
    "    \"id\": 1,\n",
    "    \"reviewers\": [\n",
    "      { \"name\": \"Ada\", \"id\": 1, \"expertise\": \"Multimedia mining\" },\n",
    "      { \"name\": \"Bob\", \"id\": 11, \"expertise\": \"Image Captioning\" },\n",
    "      { \"name\": \"Cameron\", \"id\": 12, \"expertise\": \"Image Classification\" }\n",
    "    ],\n",
    "    \"fos\": [\"Image Captioning\"],\n",
    "    \"title\": \"A Comprehensive Review of Domain-specific Image Captioning\",\n",
    "    \"year\": 2000,\n",
    "  },\n",
    "  {\n",
    "    \"id\": 2,\n",
    "    \"reviewers\": [\n",
    "      { \"name\": \"Ada\", \"id\": 1, \"expertise\": \"Multimedia mining\" },\n",
    "      { \"name\": \"David\", \"id\": 9, \"expertise\": \"Video Classification\" },\n",
    "      { \"name\": \"Cameron\", \"id\": 12, \"expertise\": \"Image Classification\" }\n",
    "    ],\n",
    "    \"fos\": [\"Machine Learning\", \"Image Segmentation\"],\n",
    "    \"title\": \"A methodology for the physically accurate visualisation of medical imaging\",\n",
    "    \"year\": 1999,\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from cmn.member import Member\n",
    "from cmn.team import Team\n",
    "\n",
    "class Review(Team):\n",
    "    def _init_(self, id, title, year, fos, reviewers):\n",
    "        super().__init__(id, reviewers, fos, year)\n",
    "        self.title = title\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_data(datapath, output, index, filter, settings):\n",
    "        try:\n",
    "            return super(Review, Review).load_data(output, index)\n",
    "        except (FileNotFoundError, EOFError) as e:\n",
    "            print(f\"Pickles not found! Reading raw data from {datapath} (progress in bytes) ...\")\n",
    "            teams = {}; candidates = {}\n",
    "\n",
    "            with open(datapath, \"r\", encoding='utf-8') as jf:\n",
    "                for line in jf:\n",
    "                    try:\n",
    "                        if not line: break\n",
    "                        jsonline = json.loads(line.lower().lstrip(\",\"))\n",
    "                        id = jsonline['id']\n",
    "                        title = jsonline['title']\n",
    "                        year = jsonline['year']\n",
    "\n",
    "                        # a team must have skills and members\n",
    "                        try: fos = jsonline['fos']\n",
    "                        except: continue\n",
    "                        try: reviewers = jsonline['reviewers']\n",
    "                        except: continue\n",
    "\n",
    "                        members = []\n",
    "                        for reviewer in reviewers:\n",
    "                            member_id = reviewer['id']\n",
    "                            member_name = reviewer['name'].replace(\" \", \"_\")\n",
    "                            if (idname := f'{member_id}_{member_name}') not in candidates:\n",
    "                                candidates[idname] = Member(member_id, member_name)\n",
    "                                candidates[idname].skills.update(set(reviewer['expertise']))\n",
    "                            members.append(candidates[idname])\n",
    "                            \n",
    "                        team = Review(id, members, title, year, members)\n",
    "                        teams[team.id] = team\n",
    "                    except json.JSONDecodeError as e:  # ideally should happen only for the last line ']'\n",
    "                        print(f'JSONDecodeError: There has been error in loading json line `{line}`!\\n{e}')\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        raise e\n",
    "            return super(Review, Review).read_data(teams, output, filter, settings)\n",
    "        except Exception as e: raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structure\n",
    "![Class Diagram of the Model baseline.](./new-class-diagram.png)\n",
    "\n",
    "To integrate a new model into the baseline, follow the `Ntf` class. Ideally, only the `learn()` method should be overriden, with `eval()` remaining the same for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdl.ntf import Ntf\n",
    "from mdl.cds import TFDataset\n",
    "from mdl.fnn import Fnn\n",
    "from cmn.team import Team\n",
    "from cmn.tools import merge_teams_by_skills\n",
    "\n",
    "class Random(Ntf):\n",
    "    def __init__(self):\n",
    "        super(Random, self).__init__()\n",
    "    \n",
    "    def init(self):\n",
    "    \n",
    "    def learn(self, splits, indexes, vecs, params, prev_model, output):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
